{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b7719d",
   "metadata": {},
   "source": [
    "# å¯¹æ¦‚å¿µçš„ç†è§£å’Œä¸¾ä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5a0ad",
   "metadata": {},
   "source": [
    "\n",
    "### Character (å­—ç¬¦)\n",
    "- **å®šä¹‰**: äººç±»å¯è¯»çš„æ–‡æœ¬å•ä½\n",
    "- **ç¤ºä¾‹**: `'h'`, `'e'`, `'ä¸­'`, `'æ–‡'`, `'ğŸ˜€'`\n",
    "- **Python ç±»å‹**: `str`\n",
    "- **ç‰¹ç‚¹**: \n",
    "  - Unicode å­—ç¬¦æ•°é‡å·¨å¤§ (è¶…è¿‡ 100 ä¸‡ç§)\n",
    "  - ä¸åŒå­—ç¬¦çš„ Unicode ç ç‚¹ä¸åŒ\n",
    "  - å­—ç¬¦æ•° â‰  å­—èŠ‚æ•°\n",
    "\n",
    "### Byte (å­—èŠ‚)\n",
    "- **å®šä¹‰**: è®¡ç®—æœºå­˜å‚¨çš„åŸºæœ¬å•ä½\n",
    "- **èŒƒå›´**: 0-255 (2^8 = 256 ç§å¯èƒ½)\n",
    "- **Python ç±»å‹**: `bytes`\n",
    "- **ç‰¹ç‚¹**:\n",
    "  - å›ºå®šèŒƒå›´: 0-255\n",
    "  - 1 å­—èŠ‚ = 8 ä½ (bit)\n",
    "  - æ‰€æœ‰æ–‡æœ¬éƒ½å¯ä»¥è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—\n",
    "\n",
    "### Unicode\n",
    "- **å®šä¹‰**ï¼šå­—ç¬¦ç¼–ç æ ‡å‡†ï¼Œä¸ºæ¯ä¸ªå­—ç¬¦åˆ†é…å”¯ä¸€çš„ç ç‚¹ï¼ˆæ ‡å‡†ï¼‰\n",
    "- **Unicode ç ç‚¹ (Code Point)**: æ¯ä¸ªå­—ç¬¦éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ Unicode ç ç‚¹ï¼Œé€šå¸¸ç”¨ `U+` å‰ç¼€è¡¨ç¤ºï¼ˆåå…­è¿›åˆ¶ï¼‰ï¼Œä¾‹å¦‚: `U+0068`, `U+4E2D`, `U+1F600`\n",
    "- **ç¤ºä¾‹**: \n",
    "```python\n",
    "ord('h')    # 104 (U+0068) \n",
    "ord('ä¸­')   # 20013 (U+4E2D)\n",
    "```\n",
    "- **Python å‡½æ•°**: ord(char)\n",
    "\n",
    "\n",
    "### UTF-8 ç¼–ç \n",
    "\n",
    "- **å®šä¹‰**: å°† Unicode ç ç‚¹ç¼–ç æˆå­—èŠ‚åºåˆ—çš„æ–¹å¼(è®¡ç®—æœºéœ€è¦å­—èŠ‚åºåˆ—æ¥å­˜å‚¨)\n",
    "\n",
    "\n",
    "- **ç¤ºä¾‹**:\n",
    "```python\n",
    "'h'.encode('utf-8')    # b'h' (1 å­—èŠ‚)\n",
    "'ä¸­'.encode('utf-8')   # b'\\xe4\\xb8\\xad' (3 å­—èŠ‚)\n",
    "'ğŸ˜€'.encode('utf-8')   # b'\\xf0\\x9f\\x98\\x80' (4 å­—èŠ‚)\n",
    "```\n",
    "\n",
    "- **Python æ–¹æ³•**: char.encode('utf-8')\n",
    "\n",
    "## å­—ç¬¦ä¸å­—èŠ‚çš„å…³ç³»\n",
    "\n",
    "```\n",
    "Character (å­—ç¬¦)\n",
    "    â†“\n",
    "Unicode ç ç‚¹ (æ•°å­—æ ‡è¯†ç¬¦)\n",
    "    â†“\n",
    "UTF-8 ç¼–ç  (å­—èŠ‚åºåˆ—)\n",
    "```\n",
    "\n",
    "### ç¤ºä¾‹: `'ä¸­'`\n",
    "\n",
    "```\n",
    "å­—ç¬¦: 'ä¸­'\n",
    "    â†“\n",
    "Unicode ç ç‚¹: 20013 (U+4E2D)\n",
    "    â†“\n",
    "UTF-8 ç¼–ç : b'\\xe4\\xb8\\xad' (3 å­—èŠ‚)\n",
    "    â†“\n",
    "å­—èŠ‚å€¼: [228, 184, 173]\n",
    "```\n",
    "\n",
    "æ¦‚å¿µå®¹æ˜“æ··æ·†ï¼š\n",
    "- Unicode ä¸æ˜¯å­—ç¬¦ï¼ŒUnicode æ˜¯å­—ç¬¦ç¼–ç æ ‡å‡†ï¼Œä¸ºå­—ç¬¦åˆ†é…ç ç‚¹ï¼Œæ˜¯å­—ç¬¦çš„æ•°å­—è¡¨ç¤ºã€‚\n",
    "- Unicode å’Œ UTF-8 ä¸æ˜¯ä¸€å›äº‹ï¼ŒUnicode æ˜¯æ ‡å‡†ï¼Œï¼ˆå®šä¹‰å­—ç¬¦å’Œç ç‚¹ï¼‰ï¼ŒUTF-8 æ˜¯ç¼–ç æ–¹å¼ï¼ˆå°†ç ç‚¹ç¼–ç æˆå­—èŠ‚ï¼‰ã€‚\n",
    "\n",
    "\n",
    "\n",
    "### å­—èŠ‚çº§åˆ«çš„ä¼˜åŠ¿\n",
    "\n",
    "1. **åˆå§‹è¯æ±‡è¡¨å¤§å°å›ºå®š**\n",
    "   - å­—ç¬¦çº§åˆ«: éœ€è¦å¤„ç†æ‰€æœ‰ Unicode å­—ç¬¦ (100ä¸‡+)\n",
    "   - å­—èŠ‚çº§åˆ«: åªéœ€è¦ 256 ä¸ªåˆå§‹ token (0-255)\n",
    "\n",
    "2. **è¦†ç›–æ€§**\n",
    "   - å­—ç¬¦çº§åˆ«: æ— æ³•å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„å­—ç¬¦\n",
    "   - å­—èŠ‚çº§åˆ«: å¯ä»¥å¤„ç†ä»»ä½• Unicode å­—ç¬¦\n",
    "\n",
    "3. **ç»Ÿä¸€æ€§**\n",
    "   - å­—ç¬¦çº§åˆ«: ä¸åŒè¯­è¨€çš„å­—ç¬¦é›†å·®å¼‚å¾ˆå¤§\n",
    "   - å­—èŠ‚çº§åˆ«: æ‰€æœ‰æ–‡æœ¬éƒ½å¯ä»¥è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—\n",
    "\n",
    "4. **ç®€å•æ€§**\n",
    "   - å­—èŠ‚èŒƒå›´å›ºå®š (0-255)\n",
    "   - å¯ä»¥å­¦ä¹ å¸¸è§å­—èŠ‚ç»„åˆçš„æ¨¡å¼\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a957c",
   "metadata": {},
   "source": [
    "# åˆ†è¯å™¨è®­ç»ƒå‡ºç°çš„æ¦‚å¿µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01466695",
   "metadata": {},
   "source": [
    "\n",
    "### 1. vocab (è¯æ±‡è¡¨)\n",
    "\n",
    "**ç±»å‹**: `dict[int, bytes]`\n",
    "\n",
    "**å«ä¹‰**: ä» token ID (æ•´æ•°) æ˜ å°„åˆ° token çš„å­—èŠ‚è¡¨ç¤º\n",
    "\n",
    "**ç»“æ„**:\n",
    "```python\n",
    "vocab = {\n",
    "    0: b'\\x00',        # token ID 0 -> å­—èŠ‚ 0\n",
    "    1: b'\\x01',        # token ID 1 -> å­—èŠ‚ 1\n",
    "    ...\n",
    "    104: b'h',         # token ID 104 -> 'h' å­—èŠ‚\n",
    "    256: b'he',        # token ID 256 -> 'he' (BPEåˆå¹¶åçš„token)\n",
    "    257: b'll',        # token ID 257 -> 'll' (BPEåˆå¹¶åçš„token)\n",
    "    258: b'hello',     # token ID 258 -> 'hello' (BPEåˆå¹¶åçš„token)\n",
    "}\n",
    "```\n",
    "\n",
    "**åºåˆ—åŒ–å** (ç”¨äºä¿å­˜åˆ° JSON):\n",
    "```python\n",
    "vocab_serialized = {\n",
    "    'Ä€': 0,           # å­—èŠ‚ 0 å¯¹åº”çš„ unicode å­—ç¬¦\n",
    "    'Ä': 1,           # å­—èŠ‚ 1 å¯¹åº”çš„ unicode å­—ç¬¦\n",
    "    'Ä ': 32,          # ç©ºæ ¼å­—èŠ‚å¯¹åº”çš„ unicode å­—ç¬¦\n",
    "    'h': 104,         # 'h' å­—èŠ‚\n",
    "    'he': 256,        # 'he' token\n",
    "    'll': 257,        # 'll' token\n",
    "    'hello': 258,     # 'hello' token\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. merges (åˆå¹¶è§„åˆ™)\n",
    "\n",
    "**ç±»å‹**: `list[tuple[bytes, bytes]]`\n",
    "\n",
    "**å«ä¹‰**: BPEè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŒ‰é¡ºåºè®°å½•çš„åˆå¹¶æ“ä½œã€‚æ¯ä¸ªå…ƒç»„ `(token1, token2)` è¡¨ç¤ºå°† `token1` å’Œ `token2` åˆå¹¶æˆä¸€ä¸ªæ–°çš„ tokenã€‚\n",
    "\n",
    "**ç»“æ„**:\n",
    "```python\n",
    "merges = [\n",
    "    (b'h', b'e'),         # ç¬¬1æ¬¡åˆå¹¶: h + e -> he\n",
    "    (b'l', b'l'),         # ç¬¬2æ¬¡åˆå¹¶: l + l -> ll\n",
    "    (b'he', b'll'),       # ç¬¬3æ¬¡åˆå¹¶: he + ll -> hell\n",
    "    (b'hell', b'o'),      # ç¬¬4æ¬¡åˆå¹¶: hell + o -> hello\n",
    "]\n",
    "```\n",
    "\n",
    "**åºåˆ—åŒ–å** (ç”¨äºä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶):\n",
    "```python\n",
    "merges_serialized = [\n",
    "    'h e',           # ç¬¬1æ¬¡åˆå¹¶\n",
    "    'l l',           # ç¬¬2æ¬¡åˆå¹¶\n",
    "    'he ll',         # ç¬¬3æ¬¡åˆå¹¶\n",
    "    'hell o',        # ç¬¬4æ¬¡åˆå¹¶\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ä¸ºä»€ä¹ˆéœ€è¦åºåˆ—åŒ–\n",
    "åºåˆ—åŒ–æ˜¯æŒ‡\n",
    "\n",
    "JSON ä¸æ”¯æŒ `bytes` ç±»å‹ï¼\n",
    "---\n",
    "\n",
    "## ğŸ”„ BPE åˆå¹¶è¿‡ç¨‹ç¤ºä¾‹\n",
    "\n",
    "å‡è®¾æˆ‘ä»¬è¦å¯¹å•è¯ **\"hello\"** è¿›è¡Œ BPE tokenizationï¼š\n",
    "\n",
    "### åˆå§‹çŠ¶æ€\n",
    "```\n",
    "åŸå§‹æ–‡æœ¬: \"hello\"\n",
    "UTF-8 ç¼–ç : b'hello'\n",
    "å­—èŠ‚å€¼: [104, 101, 108, 108, 111]\n",
    "åˆå§‹ tokens: [b'h', b'e', b'l', b'l', b'o']\n",
    "```\n",
    "\n",
    "### åº”ç”¨ merges (æŒ‰é¡ºåº)\n",
    "\n",
    "**æ­¥éª¤ 1**: åˆå¹¶ `b'h'` + `b'e'`\n",
    "```\n",
    "åˆå¹¶å‰: [b'h', b'e', b'l', b'l', b'o']\n",
    "åˆå¹¶å: [b'he', b'l', b'l', b'o']\n",
    "```\n",
    "\n",
    "**æ­¥éª¤ 2**: åˆå¹¶ `b'l'` + `b'l'`\n",
    "```\n",
    "åˆå¹¶å‰: [b'he', b'l', b'l', b'o']\n",
    "åˆå¹¶å: [b'he', b'll', b'o']\n",
    "```\n",
    "\n",
    "**æ­¥éª¤ 3**: åˆå¹¶ `b'he'` + `b'll'`\n",
    "```\n",
    "åˆå¹¶å‰: [b'he', b'll', b'o']\n",
    "åˆå¹¶å: [b'hell', b'o']\n",
    "```\n",
    "\n",
    "**æ­¥éª¤ 4**: åˆå¹¶ `b'hell'` + `b'o'`\n",
    "```\n",
    "åˆå¹¶å‰: [b'hell', b'o']\n",
    "åˆå¹¶å: [b'hello']\n",
    "```\n",
    "\n",
    "### æœ€ç»ˆç»“æœ\n",
    "```\n",
    "æœ€ç»ˆ tokens: [b'hello']\n",
    "æœ€ç»ˆ token ID: 258 (å‡è®¾ vocab[258] = b'hello')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¾ æ–‡ä»¶ä¿å­˜æ ¼å¼\n",
    "\n",
    "### vocab.json\n",
    "```json\n",
    "{\n",
    "  \"Ä€\": 0,\n",
    "  \"Ä\": 1,\n",
    "  \"Ä \": 32,\n",
    "  \"h\": 104,\n",
    "  \"he\": 256,\n",
    "  \"ll\": 257,\n",
    "  \"hello\": 258\n",
    "}\n",
    "```\n",
    "\n",
    "### merges.txt\n",
    "```\n",
    "h e\n",
    "l l\n",
    "he ll\n",
    "hell o\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f79e5",
   "metadata": {},
   "source": [
    "# Profile éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72cbc8",
   "metadata": {},
   "source": [
    "step 1: add the special token to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31cd3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "special_token = \"<|endoftext|>\"\n",
    "vocab.update({special_token: 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45605594",
   "metadata": {},
   "source": [
    "step 2: add 256 byte values to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3498114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256):\n",
    "    char = chr(i)\n",
    "    vocab.update({char: i + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "839c2484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 0,\n",
       " '\\x00': 1,\n",
       " '\\x01': 2,\n",
       " '\\x02': 3,\n",
       " '\\x03': 4,\n",
       " '\\x04': 5,\n",
       " '\\x05': 6,\n",
       " '\\x06': 7,\n",
       " '\\x07': 8,\n",
       " '\\x08': 9,\n",
       " '\\t': 10,\n",
       " '\\n': 11,\n",
       " '\\x0b': 12,\n",
       " '\\x0c': 13,\n",
       " '\\r': 14,\n",
       " '\\x0e': 15,\n",
       " '\\x0f': 16,\n",
       " '\\x10': 17,\n",
       " '\\x11': 18,\n",
       " '\\x12': 19,\n",
       " '\\x13': 20,\n",
       " '\\x14': 21,\n",
       " '\\x15': 22,\n",
       " '\\x16': 23,\n",
       " '\\x17': 24,\n",
       " '\\x18': 25,\n",
       " '\\x19': 26,\n",
       " '\\x1a': 27,\n",
       " '\\x1b': 28,\n",
       " '\\x1c': 29,\n",
       " '\\x1d': 30,\n",
       " '\\x1e': 31,\n",
       " '\\x1f': 32,\n",
       " ' ': 33,\n",
       " '!': 34,\n",
       " '\"': 35,\n",
       " '#': 36,\n",
       " '$': 37,\n",
       " '%': 38,\n",
       " '&': 39,\n",
       " \"'\": 40,\n",
       " '(': 41,\n",
       " ')': 42,\n",
       " '*': 43,\n",
       " '+': 44,\n",
       " ',': 45,\n",
       " '-': 46,\n",
       " '.': 47,\n",
       " '/': 48,\n",
       " '0': 49,\n",
       " '1': 50,\n",
       " '2': 51,\n",
       " '3': 52,\n",
       " '4': 53,\n",
       " '5': 54,\n",
       " '6': 55,\n",
       " '7': 56,\n",
       " '8': 57,\n",
       " '9': 58,\n",
       " ':': 59,\n",
       " ';': 60,\n",
       " '<': 61,\n",
       " '=': 62,\n",
       " '>': 63,\n",
       " '?': 64,\n",
       " '@': 65,\n",
       " 'A': 66,\n",
       " 'B': 67,\n",
       " 'C': 68,\n",
       " 'D': 69,\n",
       " 'E': 70,\n",
       " 'F': 71,\n",
       " 'G': 72,\n",
       " 'H': 73,\n",
       " 'I': 74,\n",
       " 'J': 75,\n",
       " 'K': 76,\n",
       " 'L': 77,\n",
       " 'M': 78,\n",
       " 'N': 79,\n",
       " 'O': 80,\n",
       " 'P': 81,\n",
       " 'Q': 82,\n",
       " 'R': 83,\n",
       " 'S': 84,\n",
       " 'T': 85,\n",
       " 'U': 86,\n",
       " 'V': 87,\n",
       " 'W': 88,\n",
       " 'X': 89,\n",
       " 'Y': 90,\n",
       " 'Z': 91,\n",
       " '[': 92,\n",
       " '\\\\': 93,\n",
       " ']': 94,\n",
       " '^': 95,\n",
       " '_': 96,\n",
       " '`': 97,\n",
       " 'a': 98,\n",
       " 'b': 99,\n",
       " 'c': 100,\n",
       " 'd': 101,\n",
       " 'e': 102,\n",
       " 'f': 103,\n",
       " 'g': 104,\n",
       " 'h': 105,\n",
       " 'i': 106,\n",
       " 'j': 107,\n",
       " 'k': 108,\n",
       " 'l': 109,\n",
       " 'm': 110,\n",
       " 'n': 111,\n",
       " 'o': 112,\n",
       " 'p': 113,\n",
       " 'q': 114,\n",
       " 'r': 115,\n",
       " 's': 116,\n",
       " 't': 117,\n",
       " 'u': 118,\n",
       " 'v': 119,\n",
       " 'w': 120,\n",
       " 'x': 121,\n",
       " 'y': 122,\n",
       " 'z': 123,\n",
       " '{': 124,\n",
       " '|': 125,\n",
       " '}': 126,\n",
       " '~': 127,\n",
       " '\\x7f': 128,\n",
       " '\\x80': 129,\n",
       " '\\x81': 130,\n",
       " '\\x82': 131,\n",
       " '\\x83': 132,\n",
       " '\\x84': 133,\n",
       " '\\x85': 134,\n",
       " '\\x86': 135,\n",
       " '\\x87': 136,\n",
       " '\\x88': 137,\n",
       " '\\x89': 138,\n",
       " '\\x8a': 139,\n",
       " '\\x8b': 140,\n",
       " '\\x8c': 141,\n",
       " '\\x8d': 142,\n",
       " '\\x8e': 143,\n",
       " '\\x8f': 144,\n",
       " '\\x90': 145,\n",
       " '\\x91': 146,\n",
       " '\\x92': 147,\n",
       " '\\x93': 148,\n",
       " '\\x94': 149,\n",
       " '\\x95': 150,\n",
       " '\\x96': 151,\n",
       " '\\x97': 152,\n",
       " '\\x98': 153,\n",
       " '\\x99': 154,\n",
       " '\\x9a': 155,\n",
       " '\\x9b': 156,\n",
       " '\\x9c': 157,\n",
       " '\\x9d': 158,\n",
       " '\\x9e': 159,\n",
       " '\\x9f': 160,\n",
       " '\\xa0': 161,\n",
       " 'Â¡': 162,\n",
       " 'Â¢': 163,\n",
       " 'Â£': 164,\n",
       " 'Â¤': 165,\n",
       " 'Â¥': 166,\n",
       " 'Â¦': 167,\n",
       " 'Â§': 168,\n",
       " 'Â¨': 169,\n",
       " 'Â©': 170,\n",
       " 'Âª': 171,\n",
       " 'Â«': 172,\n",
       " 'Â¬': 173,\n",
       " '\\xad': 174,\n",
       " 'Â®': 175,\n",
       " 'Â¯': 176,\n",
       " 'Â°': 177,\n",
       " 'Â±': 178,\n",
       " 'Â²': 179,\n",
       " 'Â³': 180,\n",
       " 'Â´': 181,\n",
       " 'Âµ': 182,\n",
       " 'Â¶': 183,\n",
       " 'Â·': 184,\n",
       " 'Â¸': 185,\n",
       " 'Â¹': 186,\n",
       " 'Âº': 187,\n",
       " 'Â»': 188,\n",
       " 'Â¼': 189,\n",
       " 'Â½': 190,\n",
       " 'Â¾': 191,\n",
       " 'Â¿': 192,\n",
       " 'Ã€': 193,\n",
       " 'Ã': 194,\n",
       " 'Ã‚': 195,\n",
       " 'Ãƒ': 196,\n",
       " 'Ã„': 197,\n",
       " 'Ã…': 198,\n",
       " 'Ã†': 199,\n",
       " 'Ã‡': 200,\n",
       " 'Ãˆ': 201,\n",
       " 'Ã‰': 202,\n",
       " 'ÃŠ': 203,\n",
       " 'Ã‹': 204,\n",
       " 'ÃŒ': 205,\n",
       " 'Ã': 206,\n",
       " 'Ã': 207,\n",
       " 'Ã': 208,\n",
       " 'Ã': 209,\n",
       " 'Ã‘': 210,\n",
       " 'Ã’': 211,\n",
       " 'Ã“': 212,\n",
       " 'Ã”': 213,\n",
       " 'Ã•': 214,\n",
       " 'Ã–': 215,\n",
       " 'Ã—': 216,\n",
       " 'Ã˜': 217,\n",
       " 'Ã™': 218,\n",
       " 'Ãš': 219,\n",
       " 'Ã›': 220,\n",
       " 'Ãœ': 221,\n",
       " 'Ã': 222,\n",
       " 'Ã': 223,\n",
       " 'ÃŸ': 224,\n",
       " 'Ã ': 225,\n",
       " 'Ã¡': 226,\n",
       " 'Ã¢': 227,\n",
       " 'Ã£': 228,\n",
       " 'Ã¤': 229,\n",
       " 'Ã¥': 230,\n",
       " 'Ã¦': 231,\n",
       " 'Ã§': 232,\n",
       " 'Ã¨': 233,\n",
       " 'Ã©': 234,\n",
       " 'Ãª': 235,\n",
       " 'Ã«': 236,\n",
       " 'Ã¬': 237,\n",
       " 'Ã­': 238,\n",
       " 'Ã®': 239,\n",
       " 'Ã¯': 240,\n",
       " 'Ã°': 241,\n",
       " 'Ã±': 242,\n",
       " 'Ã²': 243,\n",
       " 'Ã³': 244,\n",
       " 'Ã´': 245,\n",
       " 'Ãµ': 246,\n",
       " 'Ã¶': 247,\n",
       " 'Ã·': 248,\n",
       " 'Ã¸': 249,\n",
       " 'Ã¹': 250,\n",
       " 'Ãº': 251,\n",
       " 'Ã»': 252,\n",
       " 'Ã¼': 253,\n",
       " 'Ã½': 254,\n",
       " 'Ã¾': 255,\n",
       " 'Ã¿': 256}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf40a9",
   "metadata": {},
   "source": [
    "step 3: pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bed9cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low', 'low', 'low', 'low', 'lower', 'lower', 'widest', 'widest', 'widest', 'newest', 'newest', 'newest', 'newest', 'newest', 'newest']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'e', 's', 't'): 3,\n",
       " ('n', 'e', 'w', 'e', 's', 't'): 6}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\"\"\"\n",
    "\n",
    "\n",
    "words = text.split()\n",
    "print(words)\n",
    "from collections import Counter\n",
    "word_counts = Counter(words)\n",
    "frequency_table = dict(word_counts)\n",
    "\n",
    "frequency_table = {tuple(word): count for word, count in frequency_table.items()}\n",
    "frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc272147",
   "metadata": {},
   "source": [
    "step4: merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2c47bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o'): 7,\n",
       " ('o', 'w'): 7,\n",
       " ('w', 'e'): 8,\n",
       " ('e', 'r'): 2,\n",
       " ('w', 'i'): 3,\n",
       " ('i', 'd'): 3,\n",
       " ('d', 'e'): 3,\n",
       " ('e', 's'): 9,\n",
       " ('s', 't'): 9,\n",
       " ('n', 'e'): 6,\n",
       " ('e', 'w'): 6}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  count the frequency of all adjacent character pairs\n",
    "\n",
    "merge_tables = {}\n",
    "for word, count in frequency_table.items():\n",
    "   \n",
    "    for i in range(len(word)-1):\n",
    "     \n",
    "        char_pair = word[i:i+2]\n",
    "        if char_pair not in merge_tables:\n",
    "            merge_tables[char_pair] = count\n",
    "        else:\n",
    "            merge_tables[char_pair] += count\n",
    "    \n",
    "merge_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "461b35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair: ('e', 's') with count: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'es', 't'): 3,\n",
       " ('n', 'e', 'w', 'es', 't'): 6}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the most frequent pair\n",
    "most_frequent_pair = max(merge_tables, key=merge_tables.get)\n",
    "print(\"Most frequent pair:\", most_frequent_pair, \"with count:\", merge_tables[most_frequent_pair])\n",
    "\n",
    "# update the frequency table by merging the most frequent pair\n",
    "new_frequency_table = {}\n",
    "for word, count in frequency_table.items():\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if i < len(word) - 1 and word[i:i+2] == most_frequent_pair:\n",
    "            new_word.append(''.join(most_frequent_pair))\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word.append(word[i])\n",
    "            i += 1\n",
    "    new_frequency_table[tuple(new_word)] = count\n",
    "\n",
    "\n",
    "new_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd21ad68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o'): 7,\n",
       " ('o', 'w'): 7,\n",
       " ('w', 'e'): 2,\n",
       " ('e', 'r'): 2,\n",
       " ('w', 'i'): 3,\n",
       " ('i', 'd'): 3,\n",
       " ('d', 'es'): 3,\n",
       " ('es', 't'): 9,\n",
       " ('n', 'e'): 6,\n",
       " ('e', 'w'): 6,\n",
       " ('w', 'es'): 6}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  count the frequency of all adjacent character pairs\n",
    "merge_tables = {}\n",
    "for word, count in new_frequency_table.items():\n",
    "    \n",
    "     for i in range(len(word)-1):\n",
    "      \n",
    "          char_pair = word[i:i+2]\n",
    "          if char_pair not in merge_tables:\n",
    "                merge_tables[char_pair] = count\n",
    "          else:\n",
    "                merge_tables[char_pair] += count\n",
    "\n",
    "merge_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "265469dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair: ('es', 't') with count: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'est'): 3,\n",
       " ('n', 'e', 'w', 'est'): 6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the most frequent pair\n",
    "most_frequent_pair = max(merge_tables, key=merge_tables.get)\n",
    "print(\"Most frequent pair:\", most_frequent_pair, \"with count:\", merge_tables[most_frequent_pair])\n",
    "\n",
    "# update the frequency table by merging the most frequent pair\n",
    "frequency_table = {}\n",
    "for word, count in new_frequency_table.items():\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if i < len(word) - 1 and word[i:i+2] == most_frequent_pair:\n",
    "            new_word.append(''.join(most_frequent_pair))\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word.append(word[i])\n",
    "            i += 1\n",
    "    frequency_table[tuple(new_word)] = count\n",
    "\n",
    "\n",
    "frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1010867",
   "metadata": {},
   "source": [
    "å°†mergeè¿™ä¸ªè¿‡ç¨‹å†™æˆloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9691e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(merge_counts):\n",
    "    current_count = 1\n",
    "    while current_count <= merge_counts:\n",
    "        print(\"\\nMerge iteration:\", current_count)\n",
    "        #  count the frequency of all adjacent character pairs\n",
    "        merge_tables = {}\n",
    "        for word, count in frequency_table.items():\n",
    "            \n",
    "             for i in range(len(word)-1):\n",
    "              \n",
    "                  char_pair = word[i:i+2]\n",
    "                  if char_pair not in merge_tables:\n",
    "                        merge_tables[char_pair] = count\n",
    "                  else:\n",
    "                        merge_tables[char_pair] += count\n",
    "\n",
    "        # merge the most frequent pair\n",
    "        most_frequent_pair = max(merge_tables, key=merge_tables.get)\n",
    "        print(\"Most frequent pair:\", most_frequent_pair, \"with count:\", merge_tables[most_frequent_pair])\n",
    "\n",
    "        # update the frequency table by merging the most frequent pair\n",
    "        new_frequency_table = {}\n",
    "        for word, count in frequency_table.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i:i+2] == most_frequent_pair:\n",
    "                    new_word.append(''.join(most_frequent_pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_frequency_table[tuple(new_word)] = count\n",
    "\n",
    "        frequency_table.clear()\n",
    "        frequency_table.update(new_frequency_table)\n",
    "        current_count += 1 \n",
    "\n",
    "    return frequency_table, merge_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67e683e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merge iteration: 1\n",
      "Most frequent pair: ('lo', 'w') with count: 7\n",
      "\n",
      "Merge iteration: 2\n",
      "Most frequent pair: ('n', 'e') with count: 6\n",
      "\n",
      "Merge iteration: 3\n",
      "Most frequent pair: ('ne', 'w') with count: 6\n",
      "\n",
      "Merge iteration: 4\n",
      "Most frequent pair: ('new', 'est') with count: 6\n",
      "\n",
      "Merge iteration: 5\n",
      "Most frequent pair: ('w', 'i') with count: 3\n"
     ]
    }
   ],
   "source": [
    "frequency_table, merge_tables = merge(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6e3152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('low', 'e'): 2, ('e', 'r'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a26b500f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('low',): 5, ('low', 'e', 'r'): 2, ('wi', 'd', 'est'): 3, ('newest',): 6}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442e7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_finetune3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
